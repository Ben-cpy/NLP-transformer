{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated Transformer - 带注释的Transformer实现\n",
    "\n",
    "这是一个基于\"Attention is All You Need\"论文的完整Transformer实现。\n",
    "\n",
    "本notebook包含：\n",
    "- 完整的模型架构实现\n",
    "- 训练和推理代码\n",
    "- 可视化工具\n",
    "- 详细的中文注释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础库\n",
    "import os\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PyTorch相关\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# 数据处理和可视化\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置随机种子以保证可复现性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 核心组件实现\n",
    "\n",
    "### 2.1 Encoder-Decoder架构基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    标准的Encoder-Decoder架构\n",
    "    这是Transformer的顶层结构\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder  # 编码器\n",
    "        self.decoder = decoder  # 解码器\n",
    "        self.src_embed = src_embed  # 源语言嵌入层\n",
    "        self.tgt_embed = tgt_embed  # 目标语言嵌入层\n",
    "        self.generator = generator  # 输出生成器\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"处理masked的源序列和目标序列\"\"\"\n",
    "        # 先编码，再解码\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"编码源序列\"\"\"\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"解码目标序列\"\"\"\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    标准的线性层 + softmax生成步骤\n",
    "    用于将decoder输出转换为词汇表上的概率分布\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)  # 投影到词汇表大小\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 使用log_softmax而不是softmax，数值更稳定\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    产生N个相同的层（深拷贝）\n",
    "    用于创建多个相同的encoder/decoder层\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization层\n",
    "    对每个样本的特征维度进行归一化，而不是像Batch Norm那样对batch维度归一化\n",
    "    \"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 可学习的缩放和平移参数\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))  # gamma\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))  # beta\n",
    "        self.eps = eps  # 防止除零的小常数\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 计算最后一个维度的均值和标准差\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 归一化并应用可学习的仿射变换\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 残差连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    残差连接 + Layer Norm\n",
    "    为了代码简洁，这里先做norm再做残差连接（与原论文顺序相反，但效果相似）\n",
    "    输出: LayerNorm(x + Sublayer(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        应用残差连接到任何相同大小的子层\n",
    "        sublayer是一个函数，接受x作为输入\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Encoder实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder由N个相同的层堆叠而成\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)  # 克隆N个encoder层\n",
    "        self.norm = LayerNorm(layer.size)  # 最后的LayerNorm\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"依次通过每一层\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder层由两个子层组成：\n",
    "    1. Multi-head self-attention\n",
    "    2. Position-wise feed-forward network\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn  # 自注意力层\n",
    "        self.feed_forward = feed_forward  # 前馈网络\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)  # 两个残差连接\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"按照图1（左）的连接方式\"\"\"\n",
    "        # 第一个子层：self-attention\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # 第二个子层：feed-forward\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Decoder实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder由N个相同的层堆叠而成，带有masking\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        x: 目标序列\n",
    "        memory: encoder的输出\n",
    "        src_mask: 源序列的mask\n",
    "        tgt_mask: 目标序列的mask（防止看到未来信息）\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder层由三个子层组成：\n",
    "    1. Masked multi-head self-attention (只能看到之前的位置)\n",
    "    2. Multi-head cross-attention (关注encoder的输出)\n",
    "    3. Position-wise feed-forward network\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn  # 自注意力\n",
    "        self.src_attn = src_attn    # 交叉注意力（关注encoder输出）\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)  # 三个残差连接\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"按照图1（右）的连接方式\"\"\"\n",
    "        m = memory\n",
    "        # 第一个子层：masked self-attention\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 第二个子层：encoder-decoder attention\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        # 第三个子层：feed-forward\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Attention机制\n",
    "\n",
    "这是Transformer的核心！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    创建一个mask来隐藏后续位置的信息\n",
    "    用于decoder的自注意力中，确保位置i只能关注位置<=i的内容\n",
    "    \n",
    "    返回值是一个下三角矩阵（对角线及以下为True，其余为False）\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    # torch.triu创建上三角矩阵，diagonal=1表示对角线上方一位开始\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "    # 取反，得到下三角矩阵\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    计算Scaled Dot-Product Attention\n",
    "    \n",
    "    公式: Attention(Q, K, V) = softmax(Q*K^T / sqrt(d_k)) * V\n",
    "    \n",
    "    参数:\n",
    "        query: 查询矩阵 (batch, heads, seq_len, d_k)\n",
    "        key: 键矩阵 (batch, heads, seq_len, d_k)\n",
    "        value: 值矩阵 (batch, heads, seq_len, d_v)\n",
    "        mask: 可选的mask，用于遮蔽某些位置\n",
    "        dropout: 可选的dropout层\n",
    "    \n",
    "    返回:\n",
    "        attention输出和attention权重\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)  # d_k是key的维度\n",
    "    \n",
    "    # 1. 计算attention分数: Q * K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 2. 应用mask（如果有）\n",
    "    if mask is not None:\n",
    "        # 将mask为0的位置设置为一个很小的负数，softmax后会接近0\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # 3. 应用softmax得到attention权重\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    \n",
    "    # 4. 应用dropout（如果有）\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    \n",
    "    # 5. 用attention权重加权value\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention\n",
    "    \n",
    "    将d_model维度分成h个头，每个头的维度是d_k = d_model / h\n",
    "    多个头可以让模型关注不同位置的不同表示子空间的信息\n",
    "    \"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0  # 确保可以整除\n",
    "        \n",
    "        # 假设d_v总是等于d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        \n",
    "        # 创建4个线性层：Q、K、V的投影 + 最后的输出投影\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        \n",
    "        self.attn = None  # 保存attention权重，用于可视化\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        实现Multi-Head Attention\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # 所有h个头使用相同的mask\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1. 对Q、K、V做线性投影，并分成h个头\n",
    "        #    从 (batch, seq_len, d_model) 变为 (batch, h, seq_len, d_k)\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        \n",
    "        # 2. 在所有投影的向量上并行应用attention\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "        \n",
    "        # 3. \"Concat\"：将多个头的输出拼接起来\n",
    "        #    从 (batch, h, seq_len, d_k) 变回 (batch, seq_len, d_model)\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        \n",
    "        # 删除中间变量以节省内存\n",
    "        del query, key, value\n",
    "        \n",
    "        # 4. 应用最后的线性层\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    \n",
    "    对每个位置独立地应用两个线性变换，中间有ReLU激活\n",
    "    可以看作是两个kernel_size=1的卷积层\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)  # 第一个线性层，扩展维度\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)  # 第二个线性层，恢复维度\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 先升维+ReLU，再降维\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Embeddings和Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    标准的embedding层\n",
    "    将token索引转换为d_model维的向量\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)  # lookup table\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 乘以sqrt(d_model)是论文中的技巧，让embedding和positional encoding的scale相近\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    位置编码\n",
    "    \n",
    "    由于Transformer没有循环或卷积结构，需要注入位置信息\n",
    "    使用不同频率的sin和cos函数：\n",
    "    \n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    这样每个维度对应一个正弦波，波长从2π到10000·2π\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 在log空间计算位置编码（数值更稳定）\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        \n",
    "        # 计算除数项：10000^(2i/d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # 偶数位置用sin，奇数位置用cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        # 注册为buffer，不是模型参数，但会被保存到state_dict\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 将位置编码加到embedding上\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 完整模型构建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    从超参数构建完整的Transformer模型\n",
    "    \n",
    "    参数:\n",
    "        src_vocab: 源语言词汇表大小\n",
    "        tgt_vocab: 目标语言词汇表大小\n",
    "        N: encoder和decoder的层数（默认6）\n",
    "        d_model: 模型维度（默认512）\n",
    "        d_ff: feed-forward网络的中间维度（默认2048）\n",
    "        h: attention头数（默认8）\n",
    "        dropout: dropout概率（默认0.1）\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    \n",
    "    # 创建各个组件\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    # 组装模型\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "    \n",
    "    # 重要：使用Glorot/Xavier初始化参数\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 可视化辅助工具\n",
    "\n",
    "### 3.1 可视化Mask矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_mask(size=20):\n    \"\"\"\n    可视化subsequent mask\n    这个mask用于decoder，确保位置i只能看到位置<=i的信息\n    \"\"\"\n    mask = subsequent_mask(size)\n    plt.figure(figsize=(8, 8))\n    plt.imshow(mask[0].cpu().numpy(), cmap='YlGn', interpolation='nearest')\n    plt.title('Subsequent Mask\\n(Yellow=Visible, Green=Masked)', fontsize=14)\n    plt.xlabel('Positions that current position can attend to (Column)')\n    plt.ylabel('Current Position (Row)')\n    plt.colorbar()\n    \n    # Add explanation text\n    plt.text(size//2, -2, \n             'Diagonal and below are True (yellow): position i can attend to position <= i\\nAbove diagonal are False (green): position i cannot attend to position > i',\n             ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display mask\nvisualize_mask(20)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 可视化Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_positional_encoding(d_model=20, max_len=100):\n    \"\"\"\n    可视化位置编码的sin/cos波形\n    不同维度有不同的频率，形成独特的位置表示\n    \"\"\"\n    pe = PositionalEncoding(d_model, dropout=0)\n    y = pe.forward(torch.zeros(1, max_len, d_model))\n    \n    # Plot waveforms of different dimensions\n    plt.figure(figsize=(15, 5))\n    \n    # Select 4 different dimensions to show\n    dims_to_show = [4, 5, 6, 7]\n    \n    for dim in dims_to_show:\n        plt.plot(y[0, :, dim].numpy(), label=f'Dimension {dim}')\n    \n    plt.xlabel('Position')\n    plt.ylabel('Encoding Value')\n    plt.title('Sinusoidal Pattern of Positional Encoding\\nDifferent dimensions have different frequencies, closer positions have similar encodings')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    # Plot heatmap\n    plt.figure(figsize=(15, 8))\n    plt.imshow(y[0].numpy().T, aspect='auto', cmap='RdBu', interpolation='nearest')\n    plt.colorbar(label='Encoding Value')\n    plt.xlabel('Position')\n    plt.ylabel('Dimension')\n    plt.title('Positional Encoding Heatmap\\nEach column represents the complete encoding vector for one position')\n    plt.tight_layout()\n    plt.show()\n\nvisualize_positional_encoding()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 可视化Attention权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_attention(attn_weights, src_tokens=None, tgt_tokens=None):\n    \"\"\"\n    可视化attention权重矩阵\n    \n    参数:\n        attn_weights: attention权重矩阵 (seq_len_tgt, seq_len_src)\n        src_tokens: 源序列的token列表（可选）\n        tgt_tokens: 目标序列的token列表（可选）\n    \"\"\"\n    plt.figure(figsize=(10, 10))\n    \n    # Convert attention weights to numpy array\n    if torch.is_tensor(attn_weights):\n        attn_weights = attn_weights.detach().cpu().numpy()\n    \n    # Plot heatmap\n    sns.heatmap(attn_weights, cmap='YlOrRd', \n                xticklabels=src_tokens if src_tokens else 'auto',\n                yticklabels=tgt_tokens if tgt_tokens else 'auto',\n                cbar_kws={'label': 'Attention Weight'})\n    \n    plt.xlabel('Source Sequence Position')\n    plt.ylabel('Target Sequence Position')\n    plt.title('Attention Weight Visualization\\nDarker color indicates stronger attention')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练相关组件\n",
    "\n",
    "### 4.1 Batch类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"\n",
    "    训练时的batch对象\n",
    "    包含源序列、目标序列及其masks\n",
    "    \"\"\"\n",
    "    def __init__(self, src, tgt=None, pad=2):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            src: 源序列 (batch_size, src_len)\n",
    "            tgt: 目标序列 (batch_size, tgt_len)\n",
    "            pad: padding token的id\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        # 源序列mask：标记哪些位置不是padding\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        \n",
    "        if tgt is not None:\n",
    "            # 目标序列的输入：去掉最后一个token\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            # 目标序列的标签：去掉第一个token\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            # 目标序列mask：既要遮蔽padding，又要遮蔽future tokens\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            # 统计有效token数（不包括padding）\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"创建目标序列的mask：遮蔽padding和future positions\"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 学习率调度器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rate(step, model_size, factor, warmup):\n    \"\"\"\n    学习率调度函数\n    \n    公式: lrate = factor * (model_size^(-0.5) * min(step^(-0.5), step * warmup^(-1.5)))\n    \n    前warmup步线性增加，之后按step^(-0.5)衰减\n    \"\"\"\n    if step == 0:\n        step = 1\n    return factor * (\n        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n    )\n\ndef visualize_learning_rate_schedule():\n    \"\"\"\n    可视化不同配置下的学习率变化曲线\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    # Test different configurations\n    opts = [\n        [512, 1, 4000],  # Standard config\n        [512, 1, 8000],  # Longer warmup\n        [256, 1, 4000],  # Smaller model_size\n    ]\n    \n    labels = [\n        'Standard Config (d_model=512, warmup=4000)',\n        'Long Warmup (d_model=512, warmup=8000)',\n        'Small Model (d_model=256, warmup=4000)'\n    ]\n    \n    for opt, label in zip(opts, labels):\n        steps = list(range(1, 20000))\n        rates = [rate(step, *opt) for step in steps]\n        plt.plot(steps, rates, label=label)\n    \n    plt.xlabel('Training Steps')\n    plt.ylabel('Learning Rate')\n    plt.title('Learning Rate Schedules with Different Configurations\\nGradual decay after warmup period')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nvisualize_learning_rate_schedule()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Label Smoothing正则化\n",
    "    \n",
    "    不使用one-hot标签，而是将一部分概率分配给其他类别\n",
    "    这样可以防止模型过于自信，提高泛化能力\n",
    "    \n",
    "    例如，原本标签是[0, 1, 0, 0, 0]\n",
    "    smoothing=0.1后变成[0.025, 0.9, 0.025, 0.025, 0.025]\n",
    "    \"\"\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing  # 正确类别的概率\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        x: 模型的log概率输出 (batch_size, vocab_size)\n",
    "        target: 真实标签 (batch_size,)\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        \n",
    "        # 将smoothing的概率均匀分配给其他类（除了padding）\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        \n",
    "        # 正确类别得到更高的概率\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        \n",
    "        # padding位置的概率设为0\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        \n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_label_smoothing():\n    \"\"\"\n    可视化label smoothing的效果\n    \"\"\"\n    # Create an example\n    crit = LabelSmoothing(size=5, padding_idx=0, smoothing=0.4)\n    \n    # Simulate predictions\n    predict = torch.FloatTensor([\n        [0, 0.2, 0.7, 0.1, 0],\n        [0, 0.2, 0.7, 0.1, 0],\n        [0, 0.2, 0.7, 0.1, 0],\n    ])\n    \n    # True labels: [2, 1, 0]\n    target = torch.LongTensor([2, 1, 0])\n    \n    # Calculate loss (this will set true_dist)\n    crit(predict.log(), target)\n    \n    # Visualize\n    plt.figure(figsize=(12, 4))\n    \n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.bar(range(5), crit.true_dist[i].numpy())\n        plt.title(f'Sample {i+1}: True Label={target[i].item()}')\n        plt.xlabel('Class')\n        plt.ylabel('Target Probability')\n        plt.ylim([0, 1])\n        \n        # Mark confidence value\n        plt.axhline(y=crit.confidence, color='r', linestyle='--', \n                   label=f'Confidence={crit.confidence:.2f}')\n        plt.legend()\n    \n    plt.suptitle('Label Smoothing Effect\\nCorrect class gets high probability, other classes get small probability', \n                fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\nvisualize_label_smoothing()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 简单示例：复制任务\n",
    "\n",
    "我们先用一个简单的任务来测试模型：给定输入序列，输出相同的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"\"\"\n",
    "    生成复制任务的随机数据\n",
    "    \n",
    "    参数:\n",
    "        V: 词汇表大小\n",
    "        batch_size: 批次大小\n",
    "        nbatches: 批次数量\n",
    "    \"\"\"\n",
    "    for i in range(nbatches):\n",
    "        # 生成随机数据\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1  # 第一个token设为1（开始标记）\n",
    "        \n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        \n",
    "        yield Batch(src, tgt, pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"\n",
    "    简单的损失计算和训练函数\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), \n",
    "                y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    贪婪解码：每次选择概率最大的token\n",
    "    \n",
    "    这不是最好的解码策略（beam search更好），但实现简单\n",
    "    \"\"\"\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    \n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, \n",
    "            subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], \n",
    "            dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_copy_task_example():\n    \"\"\"\n    运行复制任务示例\n    训练一个小模型来复制输入序列\n    \"\"\"\n    V = 11  # Vocabulary size\n    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n    model = make_model(V, V, N=2)  # Use a small 2-layer model\n    \n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n    )\n    lr_scheduler = LambdaLR(\n        optimizer=optimizer,\n        lr_lambda=lambda step: rate(\n            step, model_size=model.src_embed[0].d_model, \n            factor=1.0, warmup=400\n        ),\n    )\n    \n    batch_size = 80\n    losses = []\n    \n    print(\"Starting copy task training...\")\n    model.train()\n    \n    for epoch in range(20):\n        epoch_loss = 0\n        for i, batch in enumerate(data_gen(V, batch_size, 20)):\n            out = model.forward(\n                batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n            )\n            loss, loss_node = SimpleLossCompute(\n                model.generator, criterion\n            )(out, batch.tgt_y, batch.ntokens)\n            \n            loss_node.backward()\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            lr_scheduler.step()\n            \n            epoch_loss += loss\n        \n        avg_loss = epoch_loss / 20\n        losses.append(avg_loss)\n        print(f\"Epoch {epoch+1}/20, Loss: {avg_loss:.4f}\")\n    \n    # Visualize training process\n    plt.figure(figsize=(10, 5))\n    plt.plot(losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Copy Task Training Curve')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    # Test model\n    model.eval()\n    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n    src_mask = torch.ones(1, 1, 10)\n    \n    print(\"\\nTest Results:\")\n    print(f\"Input sequence: {src[0].tolist()}\")\n    \n    result = greedy_decode(\n        model, src, src_mask, max_len=10, start_symbol=1\n    )\n    print(f\"Output sequence: {result[0].tolist()}\")\n    \n    return model\n\n# Run example\ncopy_model = run_copy_task_example()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型分析和可视化\n",
    "\n",
    "### 6.1 模型结构分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_model_structure(model):\n    \"\"\"\n    分析并打印模型结构信息\n    \"\"\"\n    print(\"=\"*60)\n    print(\"Transformer Model Structure Analysis\")\n    print(\"=\"*60)\n    \n    # Count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"\\nTotal Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\")\n    \n    # Analyze each component\n    print(\"\\nParameter Distribution by Component:\")\n    print(\"-\"*60)\n    \n    components = {\n        'Encoder': model.encoder,\n        'Decoder': model.decoder,\n        'Source Embedding': model.src_embed,\n        'Target Embedding': model.tgt_embed,\n        'Generator': model.generator,\n    }\n    \n    component_params = {}\n    for name, component in components.items():\n        params = sum(p.numel() for p in component.parameters())\n        component_params[name] = params\n        print(f\"{name:25s}: {params:10,} ({params/total_params*100:5.2f}%)\")\n    \n    # Plot parameter distribution pie chart\n    plt.figure(figsize=(10, 6))\n    plt.pie(component_params.values(), labels=component_params.keys(), \n            autopct='%1.1f%%', startangle=90)\n    plt.title('Parameter Distribution by Model Component')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\n# Create a standard-sized model for analysis\nanalysis_model = make_model(10000, 10000, N=6)\nanalyze_model_structure(analysis_model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Attention权重可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_model_attention(model, src, src_mask, tgt, tgt_mask):\n    \"\"\"\n    可视化模型中的attention权重\n    展示不同层和不同头的attention模式\n    \"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Forward pass\n        memory = model.encode(src, src_mask)\n        output = model.decode(memory, src_mask, tgt, tgt_mask)\n    \n    # Visualize encoder self-attention (first layer, first head)\n    if hasattr(model.encoder.layers[0].self_attn, 'attn'):\n        encoder_attn = model.encoder.layers[0].self_attn.attn\n        \n        if encoder_attn is not None:\n            # Take first batch, first head\n            attn_weights = encoder_attn[0, 0].cpu().numpy()\n            \n            plt.figure(figsize=(10, 8))\n            sns.heatmap(attn_weights, cmap='YlOrRd', \n                       cbar_kws={'label': 'Attention Weight'})\n            plt.xlabel('Key Position')\n            plt.ylabel('Query Position')\n            plt.title('Encoder Self-Attention (Layer 1, Head 1)\\nEach row shows attention from one position to all others')\n            plt.tight_layout()\n            plt.show()\n            \n            # Show all heads' attention\n            n_heads = encoder_attn.shape[1]\n            fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n            axes = axes.flatten()\n            \n            for i in range(min(8, n_heads)):\n                attn_head = encoder_attn[0, i].cpu().numpy()\n                sns.heatmap(attn_head, ax=axes[i], cmap='YlOrRd', \n                           cbar=False, square=True)\n                axes[i].set_title(f'Head {i+1}')\n                axes[i].set_xlabel('')\n                axes[i].set_ylabel('')\n            \n            plt.suptitle('Encoder Self-Attention - All 8 Heads\\nDifferent heads learn different attention patterns', \n                        fontsize=14)\n            plt.tight_layout()\n            plt.show()\n\n# Use copy task model for visualization\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\ntgt = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\nsrc_mask = torch.ones(1, 1, 10)\ntgt_mask = subsequent_mask(9)\n\nvisualize_model_attention(copy_model, src, src_mask, tgt, tgt_mask)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 总结\n",
    "\n",
    "### Transformer的关键要点：\n",
    "\n",
    "1. **Self-Attention机制**\n",
    "   - 允许每个位置关注序列中的所有位置\n",
    "   - 通过Q、K、V三个矩阵计算attention\n",
    "   - 使用scaling factor (1/√d_k) 稳定梯度\n",
    "\n",
    "2. **Multi-Head Attention**\n",
    "   - 将attention分成多个头\n",
    "   - 每个头学习不同的表示子空间\n",
    "   - 增强模型的表达能力\n",
    "\n",
    "3. **位置编码**\n",
    "   - 由于没有循环结构，需要显式添加位置信息\n",
    "   - 使用sin/cos函数的组合\n",
    "   - 允许模型学习相对位置关系\n",
    "\n",
    "4. **残差连接和Layer Norm**\n",
    "   - 帮助训练深层网络\n",
    "   - 稳定训练过程\n",
    "\n",
    "5. **Masked Attention**\n",
    "   - Decoder中防止看到未来信息\n",
    "   - 保持自回归特性\n",
    "\n",
    "### 下一步学习建议：\n",
    "\n",
    "1. 尝试在真实的翻译数据集上训练\n",
    "2. 实验不同的超参数配置\n",
    "3. 实现beam search解码\n",
    "4. 尝试其他的位置编码方法\n",
    "5. 探索Transformer的变体（如BERT、GPT等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 练习和实验\n",
    "\n",
    "### 练习1: 修改模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try creating models of different sizes\n# Observe changes in parameter count\n\nconfigs = [\n    {\"name\": \"Tiny\", \"N\": 2, \"d_model\": 128, \"d_ff\": 512, \"h\": 4},\n    {\"name\": \"Small\", \"N\": 4, \"d_model\": 256, \"d_ff\": 1024, \"h\": 8},\n    {\"name\": \"Base\", \"N\": 6, \"d_model\": 512, \"d_ff\": 2048, \"h\": 8},\n    {\"name\": \"Large\", \"N\": 12, \"d_model\": 768, \"d_ff\": 3072, \"h\": 12},\n]\n\nparam_counts = []\nmodel_names = []\n\nfor config in configs:\n    model = make_model(10000, 10000, \n                      N=config[\"N\"], \n                      d_model=config[\"d_model\"],\n                      d_ff=config[\"d_ff\"],\n                      h=config[\"h\"])\n    \n    params = sum(p.numel() for p in model.parameters())\n    param_counts.append(params / 1e6)  # Convert to millions\n    model_names.append(config[\"name\"])\n    \n    print(f\"{config['name']:10s}: {params:12,} parameters ({params/1e6:.2f}M)\")\n\n# Visualize\nplt.figure(figsize=(10, 6))\nplt.bar(model_names, param_counts, color=['skyblue', 'lightgreen', 'salmon', 'gold'])\nplt.ylabel('Number of Parameters (Million)')\nplt.title('Parameter Count Comparison for Different Transformer Model Sizes')\nplt.grid(True, alpha=0.3, axis='y')\n\n# Annotate values on bars\nfor i, (name, count) in enumerate(zip(model_names, param_counts)):\n    plt.text(i, count, f'{count:.1f}M', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2: 可视化不同层的表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_layer_outputs(model, src, src_mask):\n    \"\"\"\n    可视化encoder不同层的输出\n    观察表示如何逐层变化\n    \"\"\"\n    model.eval()\n    \n    # Get embedding output\n    x = model.src_embed(src)\n    \n    layer_outputs = [x[0].detach().cpu().numpy()]  # First batch\n    \n    # Forward pass through each layer\n    for i, layer in enumerate(model.encoder.layers):\n        x = layer(x, src_mask)\n        layer_outputs.append(x[0].detach().cpu().numpy())\n    \n    # Visualize\n    n_layers = len(layer_outputs)\n    fig, axes = plt.subplots(2, (n_layers + 1) // 2, figsize=(16, 6))\n    axes = axes.flatten()\n    \n    for i, output in enumerate(layer_outputs):\n        im = axes[i].imshow(output.T, aspect='auto', cmap='RdBu', \n                           interpolation='nearest')\n        axes[i].set_title(f'Layer {i} Output' if i > 0 else 'Embedding')\n        axes[i].set_xlabel('Sequence Position')\n        axes[i].set_ylabel('Feature Dimension')\n        plt.colorbar(im, ax=axes[i])\n    \n    # Hide extra subplots\n    for i in range(len(layer_outputs), len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle('Visualization of Encoder Layer Outputs\\nColor represents activation magnitude', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n# Example usage\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nsrc_mask = torch.ones(1, 1, 10)\nvisualize_layer_outputs(copy_model, src, src_mask)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "def save_model(model, path=\"transformer_model.pt\"):\n",
    "    \"\"\"\n",
    "    保存模型参数\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"模型已保存到: {path}\")\n",
    "\n",
    "# 加载模型\n",
    "def load_model(model, path=\"transformer_model.pt\"):\n",
    "    \"\"\"\n",
    "    加载模型参数\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"模型已从 {path} 加载\")\n",
    "    return model\n",
    "\n",
    "# 保存当前训练的模型\n",
    "save_model(copy_model, \"copy_task_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 参考资源\n",
    "\n",
    "- [原始论文: Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [PyTorch官方教程](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "- [Transformer相关论文集合](https://github.com/huggingface/transformers)\n",
    "\n",
    "祝你学习愉快！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}