{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated Transformer - 带注释的Transformer实现\n",
    "\n",
    "这是一个基于\"Attention is All You Need\"论文的完整Transformer实现。\n",
    "\n",
    "本notebook包含：\n",
    "- 完整的模型架构实现\n",
    "- 训练和推理代码\n",
    "- 可视化工具\n",
    "- 详细的中文注释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:33.268596Z",
     "iopub.status.busy": "2026-01-02T08:48:33.268596Z",
     "iopub.status.idle": "2026-01-02T08:48:41.333169Z",
     "shell.execute_reply": "2026-01-02T08:48:41.332164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "GPU名称: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# 基础库\n",
    "import os\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PyTorch相关\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# 数据处理和可视化\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置随机种子以保证可复现性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 核心组件实现\n",
    "\n",
    "### 2.1 Encoder-Decoder架构基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.376110Z",
     "iopub.status.busy": "2026-01-02T08:48:41.375106Z",
     "iopub.status.idle": "2026-01-02T08:48:41.380474Z",
     "shell.execute_reply": "2026-01-02T08:48:41.380474Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    标准的Encoder-Decoder架构\n",
    "    这是Transformer的顶层结构\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder  # 编码器\n",
    "        self.decoder = decoder  # 解码器\n",
    "        self.src_embed = src_embed  # 源语言嵌入层\n",
    "        self.tgt_embed = tgt_embed  # 目标语言嵌入层\n",
    "        self.generator = generator  # 输出生成器\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"处理masked的源序列和目标序列\"\"\"\n",
    "        # 先编码，再解码\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"编码源序列\"\"\"\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"解码目标序列\"\"\"\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.382552Z",
     "iopub.status.busy": "2026-01-02T08:48:41.382552Z",
     "iopub.status.idle": "2026-01-02T08:48:41.387927Z",
     "shell.execute_reply": "2026-01-02T08:48:41.386911Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    标准的线性层 + softmax生成步骤\n",
    "    用于将decoder输出转换为词汇表上的概率分布\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)  # 投影到词汇表大小\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 使用log_softmax而不是softmax，数值更稳定\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.389919Z",
     "iopub.status.busy": "2026-01-02T08:48:41.389919Z",
     "iopub.status.idle": "2026-01-02T08:48:41.394298Z",
     "shell.execute_reply": "2026-01-02T08:48:41.394298Z"
    }
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    产生N个相同的层（深拷贝）\n",
    "    用于创建多个相同的encoder/decoder层\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.396316Z",
     "iopub.status.busy": "2026-01-02T08:48:41.396316Z",
     "iopub.status.idle": "2026-01-02T08:48:41.400462Z",
     "shell.execute_reply": "2026-01-02T08:48:41.399938Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization层\n",
    "    对每个样本的特征维度进行归一化，而不是像Batch Norm那样对batch维度归一化\n",
    "    \"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 可学习的缩放和平移参数\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))  # gamma\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))  # beta\n",
    "        self.eps = eps  # 防止除零的小常数\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 计算最后一个维度的均值和标准差\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 归一化并应用可学习的仿射变换\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 残差连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.402058Z",
     "iopub.status.busy": "2026-01-02T08:48:41.402058Z",
     "iopub.status.idle": "2026-01-02T08:48:41.407322Z",
     "shell.execute_reply": "2026-01-02T08:48:41.406605Z"
    }
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    残差连接 + Layer Norm\n",
    "    为了代码简洁，这里先做norm再做残差连接（与原论文顺序相反，但效果相似）\n",
    "    输出: LayerNorm(x + Sublayer(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        应用残差连接到任何相同大小的子层\n",
    "        sublayer是一个函数，接受x作为输入\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Encoder实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.409257Z",
     "iopub.status.busy": "2026-01-02T08:48:41.409257Z",
     "iopub.status.idle": "2026-01-02T08:48:41.414754Z",
     "shell.execute_reply": "2026-01-02T08:48:41.413744Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder由N个相同的层堆叠而成\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)  # 克隆N个encoder层\n",
    "        self.norm = LayerNorm(layer.size)  # 最后的LayerNorm\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"依次通过每一层\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.419285Z",
     "iopub.status.busy": "2026-01-02T08:48:41.418274Z",
     "iopub.status.idle": "2026-01-02T08:48:41.424086Z",
     "shell.execute_reply": "2026-01-02T08:48:41.424086Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder层由两个子层组成：\n",
    "    1. Multi-head self-attention\n",
    "    2. Position-wise feed-forward network\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn  # 自注意力层\n",
    "        self.feed_forward = feed_forward  # 前馈网络\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)  # 两个残差连接\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"按照图1（左）的连接方式\"\"\"\n",
    "        # 第一个子层：self-attention\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # 第二个子层：feed-forward\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Decoder实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.426093Z",
     "iopub.status.busy": "2026-01-02T08:48:41.426093Z",
     "iopub.status.idle": "2026-01-02T08:48:41.431555Z",
     "shell.execute_reply": "2026-01-02T08:48:41.430511Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder由N个相同的层堆叠而成，带有masking\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        x: 目标序列\n",
    "        memory: encoder的输出\n",
    "        src_mask: 源序列的mask\n",
    "        tgt_mask: 目标序列的mask（防止看到未来信息）\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.433571Z",
     "iopub.status.busy": "2026-01-02T08:48:41.433571Z",
     "iopub.status.idle": "2026-01-02T08:48:41.440011Z",
     "shell.execute_reply": "2026-01-02T08:48:41.439492Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder层由三个子层组成：\n",
    "    1. Masked multi-head self-attention (只能看到之前的位置)\n",
    "    2. Multi-head cross-attention (关注encoder的输出)\n",
    "    3. Position-wise feed-forward network\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn  # 自注意力\n",
    "        self.src_attn = src_attn    # 交叉注意力（关注encoder输出）\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)  # 三个残差连接\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"按照图1（右）的连接方式\"\"\"\n",
    "        m = memory\n",
    "        # 第一个子层：masked self-attention\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 第二个子层：encoder-decoder attention\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        # 第三个子层：feed-forward\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Attention机制\n",
    "\n",
    "这是Transformer的核心！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.441524Z",
     "iopub.status.busy": "2026-01-02T08:48:41.441524Z",
     "iopub.status.idle": "2026-01-02T08:48:41.446502Z",
     "shell.execute_reply": "2026-01-02T08:48:41.446502Z"
    }
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    创建一个mask来隐藏后续位置的信息\n",
    "    用于decoder的自注意力中，确保位置i只能关注位置<=i的内容\n",
    "    \n",
    "    返回值是一个下三角矩阵（对角线及以下为True，其余为False）\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    # torch.triu创建上三角矩阵，diagonal=1表示对角线上方一位开始\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "    # 取反，得到下三角矩阵\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.448510Z",
     "iopub.status.busy": "2026-01-02T08:48:41.448510Z",
     "iopub.status.idle": "2026-01-02T08:48:41.452038Z",
     "shell.execute_reply": "2026-01-02T08:48:41.451518Z"
    }
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    计算Scaled Dot-Product Attention\n",
    "    \n",
    "    公式: Attention(Q, K, V) = softmax(Q*K^T / sqrt(d_k)) * V\n",
    "    \n",
    "    参数:\n",
    "        query: 查询矩阵 (batch, heads, seq_len, d_k)\n",
    "        key: 键矩阵 (batch, heads, seq_len, d_k)\n",
    "        value: 值矩阵 (batch, heads, seq_len, d_v)\n",
    "        mask: 可选的mask，用于遮蔽某些位置\n",
    "        dropout: 可选的dropout层\n",
    "    \n",
    "    返回:\n",
    "        attention输出和attention权重\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)  # d_k是key的维度\n",
    "    \n",
    "    # 1. 计算attention分数: Q * K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 2. 应用mask（如果有）\n",
    "    if mask is not None:\n",
    "        # 将mask为0的位置设置为一个很小的负数，softmax后会接近0\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # 3. 应用softmax得到attention权重\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    \n",
    "    # 4. 应用dropout（如果有）\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    \n",
    "    # 5. 用attention权重加权value\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.453036Z",
     "iopub.status.busy": "2026-01-02T08:48:41.453036Z",
     "iopub.status.idle": "2026-01-02T08:48:41.460135Z",
     "shell.execute_reply": "2026-01-02T08:48:41.459120Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention\n",
    "    \n",
    "    将d_model维度分成h个头，每个头的维度是d_k = d_model / h\n",
    "    多个头可以让模型关注不同位置的不同表示子空间的信息\n",
    "    \"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0  # 确保可以整除\n",
    "        \n",
    "        # 假设d_v总是等于d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        \n",
    "        # 创建4个线性层：Q、K、V的投影 + 最后的输出投影\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        \n",
    "        self.attn = None  # 保存attention权重，用于可视化\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        实现Multi-Head Attention\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # 所有h个头使用相同的mask\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1. 对Q、K、V做线性投影，并分成h个头\n",
    "        #    从 (batch, seq_len, d_model) 变为 (batch, h, seq_len, d_k)\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        \n",
    "        # 2. 在所有投影的向量上并行应用attention\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "        \n",
    "        # 3. \"Concat\"：将多个头的输出拼接起来\n",
    "        #    从 (batch, h, seq_len, d_k) 变回 (batch, seq_len, d_model)\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        \n",
    "        # 删除中间变量以节省内存\n",
    "        del query, key, value\n",
    "        \n",
    "        # 4. 应用最后的线性层\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.463683Z",
     "iopub.status.busy": "2026-01-02T08:48:41.463683Z",
     "iopub.status.idle": "2026-01-02T08:48:41.471737Z",
     "shell.execute_reply": "2026-01-02T08:48:41.471205Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    \n",
    "    对每个位置独立地应用两个线性变换，中间有ReLU激活\n",
    "    可以看作是两个kernel_size=1的卷积层\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)  # 第一个线性层，扩展维度\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)  # 第二个线性层，恢复维度\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 先升维+ReLU，再降维\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Embeddings和Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.473744Z",
     "iopub.status.busy": "2026-01-02T08:48:41.473744Z",
     "iopub.status.idle": "2026-01-02T08:48:41.481314Z",
     "shell.execute_reply": "2026-01-02T08:48:41.479788Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    标准的embedding层\n",
    "    将token索引转换为d_model维的向量\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)  # lookup table\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 乘以sqrt(d_model)是论文中的技巧，让embedding和positional encoding的scale相近\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.485325Z",
     "iopub.status.busy": "2026-01-02T08:48:41.485325Z",
     "iopub.status.idle": "2026-01-02T08:48:41.491436Z",
     "shell.execute_reply": "2026-01-02T08:48:41.491436Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    位置编码\n",
    "    \n",
    "    由于Transformer没有循环或卷积结构，需要注入位置信息\n",
    "    使用不同频率的sin和cos函数：\n",
    "    \n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    这样每个维度对应一个正弦波，波长从2π到10000·2π\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 在log空间计算位置编码（数值更稳定）\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        \n",
    "        # 计算除数项：10000^(2i/d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # 偶数位置用sin，奇数位置用cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        # 注册为buffer，不是模型参数，但会被保存到state_dict\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 将位置编码加到embedding上\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 完整模型构建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.493451Z",
     "iopub.status.busy": "2026-01-02T08:48:41.493451Z",
     "iopub.status.idle": "2026-01-02T08:48:41.498454Z",
     "shell.execute_reply": "2026-01-02T08:48:41.498454Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    从超参数构建完整的Transformer模型\n",
    "    \n",
    "    参数:\n",
    "        src_vocab: 源语言词汇表大小\n",
    "        tgt_vocab: 目标语言词汇表大小\n",
    "        N: encoder和decoder的层数（默认6）\n",
    "        d_model: 模型维度（默认512）\n",
    "        d_ff: feed-forward网络的中间维度（默认2048）\n",
    "        h: attention头数（默认8）\n",
    "        dropout: dropout概率（默认0.1）\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    \n",
    "    # 创建各个组件\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    # 组装模型\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "    \n",
    "    # 重要：使用Glorot/Xavier初始化参数\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 可视化辅助工具\n",
    "\n",
    "### 3.1 可视化Mask矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:41.499753Z",
     "iopub.status.busy": "2026-01-02T08:48:41.499753Z",
     "iopub.status.idle": "2026-01-02T08:48:42.158520Z",
     "shell.execute_reply": "2026-01-02T08:48:42.157503Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_mask(size=20):\n",
    "    \"\"\"\n",
    "    可视化subsequent mask\n",
    "    这个mask用于decoder，确保位置i只能看到位置<=i的信息\n",
    "    \"\"\"\n",
    "    mask = subsequent_mask(size)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(mask[0].cpu().numpy(), cmap='YlGn', interpolation='nearest')\n",
    "    plt.title('Subsequent Mask\\n(黄色=可见, 绿色=被遮蔽)', fontsize=14)\n",
    "    plt.xlabel('当前位置可以看到的位置 (列)')\n",
    "    plt.ylabel('当前位置 (行)')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # 添加说明文字\n",
    "    plt.text(size//2, -2, \n",
    "             '对角线及以下为True(黄色)：位置i可以看到位置<=i\\n对角线以上为False(绿色)：位置i看不到位置>i',\n",
    "             ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 显示mask\n",
    "visualize_mask(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 可视化Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:42.162044Z",
     "iopub.status.busy": "2026-01-02T08:48:42.161520Z",
     "iopub.status.idle": "2026-01-02T08:48:42.811394Z",
     "shell.execute_reply": "2026-01-02T08:48:42.810391Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_positional_encoding(d_model=20, max_len=100):\n",
    "    \"\"\"\n",
    "    可视化位置编码的sin/cos波形\n",
    "    不同维度有不同的频率，形成独特的位置表示\n",
    "    \"\"\"\n",
    "    pe = PositionalEncoding(d_model, dropout=0)\n",
    "    y = pe.forward(torch.zeros(1, max_len, d_model))\n",
    "    \n",
    "    # 绘制几个不同维度的波形\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 选择4个不同的维度来展示\n",
    "    dims_to_show = [4, 5, 6, 7]\n",
    "    \n",
    "    for dim in dims_to_show:\n",
    "        plt.plot(y[0, :, dim].numpy(), label=f'维度 {dim}')\n",
    "    \n",
    "    plt.xlabel('位置')\n",
    "    plt.ylabel('编码值')\n",
    "    plt.title('位置编码的正弦波模式\\n不同维度有不同频率，位置越近，编码越相似')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 绘制热力图\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(y[0].numpy().T, aspect='auto', cmap='RdBu', interpolation='nearest')\n",
    "    plt.colorbar(label='编码值')\n",
    "    plt.xlabel('位置')\n",
    "    plt.ylabel('维度')\n",
    "    plt.title('位置编码热力图\\n每一列代表一个位置的完整编码向量')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 可视化Attention权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:42.814415Z",
     "iopub.status.busy": "2026-01-02T08:48:42.813415Z",
     "iopub.status.idle": "2026-01-02T08:48:42.819439Z",
     "shell.execute_reply": "2026-01-02T08:48:42.819439Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_attention(attn_weights, src_tokens=None, tgt_tokens=None):\n",
    "    \"\"\"\n",
    "    可视化attention权重矩阵\n",
    "    \n",
    "    参数:\n",
    "        attn_weights: attention权重矩阵 (seq_len_tgt, seq_len_src)\n",
    "        src_tokens: 源序列的token列表（可选）\n",
    "        tgt_tokens: 目标序列的token列表（可选）\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # 将attention权重转为numpy数组\n",
    "    if torch.is_tensor(attn_weights):\n",
    "        attn_weights = attn_weights.detach().cpu().numpy()\n",
    "    \n",
    "    # 绘制热力图\n",
    "    sns.heatmap(attn_weights, cmap='YlOrRd', \n",
    "                xticklabels=src_tokens if src_tokens else 'auto',\n",
    "                yticklabels=tgt_tokens if tgt_tokens else 'auto',\n",
    "                cbar_kws={'label': 'Attention权重'})\n",
    "    \n",
    "    plt.xlabel('源序列位置')\n",
    "    plt.ylabel('目标序列位置')\n",
    "    plt.title('Attention权重可视化\\n颜色越深，attention越强')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练相关组件\n",
    "\n",
    "### 4.1 Batch类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:42.822629Z",
     "iopub.status.busy": "2026-01-02T08:48:42.822629Z",
     "iopub.status.idle": "2026-01-02T08:48:42.828853Z",
     "shell.execute_reply": "2026-01-02T08:48:42.827843Z"
    }
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"\n",
    "    训练时的batch对象\n",
    "    包含源序列、目标序列及其masks\n",
    "    \"\"\"\n",
    "    def __init__(self, src, tgt=None, pad=2):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            src: 源序列 (batch_size, src_len)\n",
    "            tgt: 目标序列 (batch_size, tgt_len)\n",
    "            pad: padding token的id\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        # 源序列mask：标记哪些位置不是padding\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        \n",
    "        if tgt is not None:\n",
    "            # 目标序列的输入：去掉最后一个token\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            # 目标序列的标签：去掉第一个token\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            # 目标序列mask：既要遮蔽padding，又要遮蔽future tokens\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            # 统计有效token数（不包括padding）\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"创建目标序列的mask：遮蔽padding和future positions\"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 学习率调度器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:42.831368Z",
     "iopub.status.busy": "2026-01-02T08:48:42.831368Z",
     "iopub.status.idle": "2026-01-02T08:48:43.240177Z",
     "shell.execute_reply": "2026-01-02T08:48:43.239166Z"
    }
   },
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    学习率调度函数\n",
    "    \n",
    "    公式: lrate = factor * (model_size^(-0.5) * min(step^(-0.5), step * warmup^(-1.5)))\n",
    "    \n",
    "    前warmup步线性增加，之后按step^(-0.5)衰减\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )\n",
    "\n",
    "def visualize_learning_rate_schedule():\n",
    "    \"\"\"\n",
    "    可视化不同配置下的学习率变化曲线\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 测试不同的配置\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # 标准配置\n",
    "        [512, 1, 8000],  # 更长的warmup\n",
    "        [256, 1, 4000],  # 更小的model_size\n",
    "    ]\n",
    "    \n",
    "    labels = [\n",
    "        '标准配置 (d_model=512, warmup=4000)',\n",
    "        '长warmup (d_model=512, warmup=8000)',\n",
    "        '小模型 (d_model=256, warmup=4000)'\n",
    "    ]\n",
    "    \n",
    "    for opt, label in zip(opts, labels):\n",
    "        steps = list(range(1, 20000))\n",
    "        rates = [rate(step, *opt) for step in steps]\n",
    "        plt.plot(steps, rates, label=label)\n",
    "    \n",
    "    plt.xlabel('训练步数')\n",
    "    plt.ylabel('学习率')\n",
    "    plt.title('不同配置下的学习率调度\\nWarmup后逐渐衰减')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_learning_rate_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:43.242696Z",
     "iopub.status.busy": "2026-01-02T08:48:43.242696Z",
     "iopub.status.idle": "2026-01-02T08:48:43.250221Z",
     "shell.execute_reply": "2026-01-02T08:48:43.249704Z"
    }
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Label Smoothing正则化\n",
    "    \n",
    "    不使用one-hot标签，而是将一部分概率分配给其他类别\n",
    "    这样可以防止模型过于自信，提高泛化能力\n",
    "    \n",
    "    例如，原本标签是[0, 1, 0, 0, 0]\n",
    "    smoothing=0.1后变成[0.025, 0.9, 0.025, 0.025, 0.025]\n",
    "    \"\"\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing  # 正确类别的概率\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        x: 模型的log概率输出 (batch_size, vocab_size)\n",
    "        target: 真实标签 (batch_size,)\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        \n",
    "        # 将smoothing的概率均匀分配给其他类（除了padding）\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        \n",
    "        # 正确类别得到更高的概率\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        \n",
    "        # padding位置的概率设为0\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        \n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:43.252629Z",
     "iopub.status.busy": "2026-01-02T08:48:43.252629Z",
     "iopub.status.idle": "2026-01-02T08:48:43.775272Z",
     "shell.execute_reply": "2026-01-02T08:48:43.774257Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_label_smoothing():\n",
    "    \"\"\"\n",
    "    可视化label smoothing的效果\n",
    "    \"\"\"\n",
    "    # 创建一个示例\n",
    "    crit = LabelSmoothing(size=5, padding_idx=0, smoothing=0.4)\n",
    "    \n",
    "    # 模拟预测\n",
    "    predict = torch.FloatTensor([\n",
    "        [0, 0.2, 0.7, 0.1, 0],\n",
    "        [0, 0.2, 0.7, 0.1, 0],\n",
    "        [0, 0.2, 0.7, 0.1, 0],\n",
    "    ])\n",
    "    \n",
    "    # 真实标签: [2, 1, 0]\n",
    "    target = torch.LongTensor([2, 1, 0])\n",
    "    \n",
    "    # 计算loss（会设置true_dist）\n",
    "    crit(predict.log(), target)\n",
    "    \n",
    "    # 可视化\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.bar(range(5), crit.true_dist[i].numpy())\n",
    "        plt.title(f'样本{i+1}: 真实标签={target[i].item()}')\n",
    "        plt.xlabel('类别')\n",
    "        plt.ylabel('目标概率')\n",
    "        plt.ylim([0, 1])\n",
    "        \n",
    "        # 标注confidence值\n",
    "        plt.axhline(y=crit.confidence, color='r', linestyle='--', \n",
    "                   label=f'Confidence={crit.confidence:.2f}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.suptitle('Label Smoothing效果\\n正确类别获得高概率，其他类别分配少量概率', \n",
    "                fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_label_smoothing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 简单示例：复制任务\n",
    "\n",
    "我们先用一个简单的任务来测试模型：给定输入序列，输出相同的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:43.778267Z",
     "iopub.status.busy": "2026-01-02T08:48:43.777272Z",
     "iopub.status.idle": "2026-01-02T08:48:43.783331Z",
     "shell.execute_reply": "2026-01-02T08:48:43.782316Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"\"\"\n",
    "    生成复制任务的随机数据\n",
    "    \n",
    "    参数:\n",
    "        V: 词汇表大小\n",
    "        batch_size: 批次大小\n",
    "        nbatches: 批次数量\n",
    "    \"\"\"\n",
    "    for i in range(nbatches):\n",
    "        # 生成随机数据\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1  # 第一个token设为1（开始标记）\n",
    "        \n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        \n",
    "        yield Batch(src, tgt, pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:43.786330Z",
     "iopub.status.busy": "2026-01-02T08:48:43.785330Z",
     "iopub.status.idle": "2026-01-02T08:48:43.792398Z",
     "shell.execute_reply": "2026-01-02T08:48:43.789864Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"\n",
    "    简单的损失计算和训练函数\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), \n",
    "                y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:43.794398Z",
     "iopub.status.busy": "2026-01-02T08:48:43.794398Z",
     "iopub.status.idle": "2026-01-02T08:48:43.799917Z",
     "shell.execute_reply": "2026-01-02T08:48:43.799399Z"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    贪婪解码：每次选择概率最大的token\n",
    "    \n",
    "    这不是最好的解码策略（beam search更好），但实现简单\n",
    "    \"\"\"\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    \n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, \n",
    "            subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], \n",
    "            dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:48:43.802443Z",
     "iopub.status.busy": "2026-01-02T08:48:43.802443Z",
     "iopub.status.idle": "2026-01-02T08:53:52.152397Z",
     "shell.execute_reply": "2026-01-02T08:53:52.151381Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_copy_task_example():\n",
    "    \"\"\"\n",
    "    运行复制任务示例\n",
    "    训练一个小模型来复制输入序列\n",
    "    \"\"\"\n",
    "    V = 11  # 词汇表大小\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(V, V, N=2)  # 使用2层的小模型\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, \n",
    "            factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    batch_size = 80\n",
    "    losses = []\n",
    "    \n",
    "    print(\"开始训练复制任务...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        epoch_loss = 0\n",
    "        for i, batch in enumerate(data_gen(V, batch_size, 20)):\n",
    "            out = model.forward(\n",
    "                batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "            )\n",
    "            loss, loss_node = SimpleLossCompute(\n",
    "                model.generator, criterion\n",
    "            )(out, batch.tgt_y, batch.ntokens)\n",
    "            \n",
    "            loss_node.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "        \n",
    "        avg_loss = epoch_loss / 20\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/20, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 可视化训练过程\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('复制任务训练曲线')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "    \n",
    "    print(\"\\n测试结果:\")\n",
    "    print(f\"输入序列: {src[0].tolist()}\")\n",
    "    \n",
    "    result = greedy_decode(\n",
    "        model, src, src_mask, max_len=10, start_symbol=1\n",
    "    )\n",
    "    print(f\"输出序列: {result[0].tolist()}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 运行示例\n",
    "copy_model = run_copy_task_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型分析和可视化\n",
    "\n",
    "### 6.1 模型结构分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:53:52.155487Z",
     "iopub.status.busy": "2026-01-02T08:53:52.154987Z",
     "iopub.status.idle": "2026-01-02T08:53:52.779279Z",
     "shell.execute_reply": "2026-01-02T08:53:52.777901Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_model_structure(model):\n",
    "    \"\"\"\n",
    "    分析并打印模型结构信息\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer模型结构分析\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 统计参数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n总参数量: {total_params:,}\")\n",
    "    print(f\"可训练参数量: {trainable_params:,}\")\n",
    "    \n",
    "    # 分析各个组件\n",
    "    print(\"\\n各组件参数分布:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    components = {\n",
    "        'Encoder': model.encoder,\n",
    "        'Decoder': model.decoder,\n",
    "        'Source Embedding': model.src_embed,\n",
    "        'Target Embedding': model.tgt_embed,\n",
    "        'Generator': model.generator,\n",
    "    }\n",
    "    \n",
    "    component_params = {}\n",
    "    for name, component in components.items():\n",
    "        params = sum(p.numel() for p in component.parameters())\n",
    "        component_params[name] = params\n",
    "        print(f\"{name:25s}: {params:10,} ({params/total_params*100:5.2f}%)\")\n",
    "    \n",
    "    # 绘制参数分布饼图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.pie(component_params.values(), labels=component_params.keys(), \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('模型各组件参数分布')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 创建一个标准大小的模型进行分析\n",
    "analysis_model = make_model(10000, 10000, N=6)\n",
    "analyze_model_structure(analysis_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Attention权重可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:53:52.782318Z",
     "iopub.status.busy": "2026-01-02T08:53:52.781864Z",
     "iopub.status.idle": "2026-01-02T08:53:55.775074Z",
     "shell.execute_reply": "2026-01-02T08:53:55.774059Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_model_attention(model, src, src_mask, tgt, tgt_mask):\n",
    "    \"\"\"\n",
    "    可视化模型中的attention权重\n",
    "    展示不同层和不同头的attention模式\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 前向传播\n",
    "        memory = model.encode(src, src_mask)\n",
    "        output = model.decode(memory, src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    # 可视化encoder的self-attention（第一层，第一个头）\n",
    "    if hasattr(model.encoder.layers[0].self_attn, 'attn'):\n",
    "        encoder_attn = model.encoder.layers[0].self_attn.attn\n",
    "        \n",
    "        if encoder_attn is not None:\n",
    "            # 取第一个batch，第一个头\n",
    "            attn_weights = encoder_attn[0, 0].cpu().numpy()\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(attn_weights, cmap='YlOrRd', \n",
    "                       cbar_kws={'label': 'Attention权重'})\n",
    "            plt.xlabel('Key位置')\n",
    "            plt.ylabel('Query位置')\n",
    "            plt.title('Encoder Self-Attention (第1层, 第1个头)\\n每行显示一个位置对其他位置的attention')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 显示所有头的attention\n",
    "            n_heads = encoder_attn.shape[1]\n",
    "            fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i in range(min(8, n_heads)):\n",
    "                attn_head = encoder_attn[0, i].cpu().numpy()\n",
    "                sns.heatmap(attn_head, ax=axes[i], cmap='YlOrRd', \n",
    "                           cbar=False, square=True)\n",
    "                axes[i].set_title(f'Head {i+1}')\n",
    "                axes[i].set_xlabel('')\n",
    "                axes[i].set_ylabel('')\n",
    "            \n",
    "            plt.suptitle('Encoder Self-Attention - 所有8个头\\n不同的头学习到不同的attention模式', \n",
    "                        fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# 使用复制任务的模型进行可视化\n",
    "src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "tgt = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "src_mask = torch.ones(1, 1, 10)\n",
    "tgt_mask = subsequent_mask(9)\n",
    "\n",
    "visualize_model_attention(copy_model, src, src_mask, tgt, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 总结\n",
    "\n",
    "### Transformer的关键要点：\n",
    "\n",
    "1. **Self-Attention机制**\n",
    "   - 允许每个位置关注序列中的所有位置\n",
    "   - 通过Q、K、V三个矩阵计算attention\n",
    "   - 使用scaling factor (1/√d_k) 稳定梯度\n",
    "\n",
    "2. **Multi-Head Attention**\n",
    "   - 将attention分成多个头\n",
    "   - 每个头学习不同的表示子空间\n",
    "   - 增强模型的表达能力\n",
    "\n",
    "3. **位置编码**\n",
    "   - 由于没有循环结构，需要显式添加位置信息\n",
    "   - 使用sin/cos函数的组合\n",
    "   - 允许模型学习相对位置关系\n",
    "\n",
    "4. **残差连接和Layer Norm**\n",
    "   - 帮助训练深层网络\n",
    "   - 稳定训练过程\n",
    "\n",
    "5. **Masked Attention**\n",
    "   - Decoder中防止看到未来信息\n",
    "   - 保持自回归特性\n",
    "\n",
    "### 下一步学习建议：\n",
    "\n",
    "1. 尝试在真实的翻译数据集上训练\n",
    "2. 实验不同的超参数配置\n",
    "3. 实现beam search解码\n",
    "4. 尝试其他的位置编码方法\n",
    "5. 探索Transformer的变体（如BERT、GPT等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 练习和实验\n",
    "\n",
    "### 练习1: 修改模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:53:55.777068Z",
     "iopub.status.busy": "2026-01-02T08:53:55.777068Z",
     "iopub.status.idle": "2026-01-02T08:53:58.439258Z",
     "shell.execute_reply": "2026-01-02T08:53:58.438098Z"
    }
   },
   "outputs": [],
   "source": [
    "# 尝试创建不同大小的模型\n",
    "# 观察参数量的变化\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"Tiny\", \"N\": 2, \"d_model\": 128, \"d_ff\": 512, \"h\": 4},\n",
    "    {\"name\": \"Small\", \"N\": 4, \"d_model\": 256, \"d_ff\": 1024, \"h\": 8},\n",
    "    {\"name\": \"Base\", \"N\": 6, \"d_model\": 512, \"d_ff\": 2048, \"h\": 8},\n",
    "    {\"name\": \"Large\", \"N\": 12, \"d_model\": 768, \"d_ff\": 3072, \"h\": 12},\n",
    "]\n",
    "\n",
    "param_counts = []\n",
    "model_names = []\n",
    "\n",
    "for config in configs:\n",
    "    model = make_model(10000, 10000, \n",
    "                      N=config[\"N\"], \n",
    "                      d_model=config[\"d_model\"],\n",
    "                      d_ff=config[\"d_ff\"],\n",
    "                      h=config[\"h\"])\n",
    "    \n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    param_counts.append(params / 1e6)  # 转换为百万\n",
    "    model_names.append(config[\"name\"])\n",
    "    \n",
    "    print(f\"{config['name']:10s}: {params:12,} parameters ({params/1e6:.2f}M)\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, param_counts, color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
    "plt.ylabel('参数量 (百万)')\n",
    "plt.title('不同规模Transformer模型的参数量对比')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 在柱子上标注数值\n",
    "for i, (name, count) in enumerate(zip(model_names, param_counts)):\n",
    "    plt.text(i, count, f'{count:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2: 可视化不同层的表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:53:58.441965Z",
     "iopub.status.busy": "2026-01-02T08:53:58.441560Z",
     "iopub.status.idle": "2026-01-02T08:53:59.078446Z",
     "shell.execute_reply": "2026-01-02T08:53:59.077439Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_layer_outputs(model, src, src_mask):\n",
    "    \"\"\"\n",
    "    可视化encoder不同层的输出\n",
    "    观察表示如何逐层变化\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 获取embedding输出\n",
    "    x = model.src_embed(src)\n",
    "    \n",
    "    layer_outputs = [x[0].detach().cpu().numpy()]  # 第一个batch\n",
    "    \n",
    "    # 逐层前向传播\n",
    "    for i, layer in enumerate(model.encoder.layers):\n",
    "        x = layer(x, src_mask)\n",
    "        layer_outputs.append(x[0].detach().cpu().numpy())\n",
    "    \n",
    "    # 可视化\n",
    "    n_layers = len(layer_outputs)\n",
    "    fig, axes = plt.subplots(2, (n_layers + 1) // 2, figsize=(16, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, output in enumerate(layer_outputs):\n",
    "        im = axes[i].imshow(output.T, aspect='auto', cmap='RdBu', \n",
    "                           interpolation='nearest')\n",
    "        axes[i].set_title(f'Layer {i}输出' if i > 0 else 'Embedding')\n",
    "        axes[i].set_xlabel('序列位置')\n",
    "        axes[i].set_ylabel('特征维度')\n",
    "        plt.colorbar(im, ax=axes[i])\n",
    "    \n",
    "    # 隐藏多余的子图\n",
    "    for i in range(len(layer_outputs), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Encoder各层输出的可视化\\n颜色表示激活值的大小', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例\n",
    "src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "src_mask = torch.ones(1, 1, 10)\n",
    "visualize_layer_outputs(copy_model, src, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T08:53:59.080883Z",
     "iopub.status.busy": "2026-01-02T08:53:59.080883Z",
     "iopub.status.idle": "2026-01-02T08:53:59.490107Z",
     "shell.execute_reply": "2026-01-02T08:53:59.489089Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "def save_model(model, path=\"transformer_model.pt\"):\n",
    "    \"\"\"\n",
    "    保存模型参数\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"模型已保存到: {path}\")\n",
    "\n",
    "# 加载模型\n",
    "def load_model(model, path=\"transformer_model.pt\"):\n",
    "    \"\"\"\n",
    "    加载模型参数\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"模型已从 {path} 加载\")\n",
    "    return model\n",
    "\n",
    "# 保存当前训练的模型\n",
    "save_model(copy_model, \"copy_task_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 参考资源\n",
    "\n",
    "- [原始论文: Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [PyTorch官方教程](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "- [Transformer相关论文集合](https://github.com/huggingface/transformers)\n",
    "\n",
    "祝你学习愉快！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
